{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e1064",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers peft datasets evaluate bitsandbytes torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43bed6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda4eb1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CAPTION_FILE = \"/mnt/data/100_ex.csv\"\n",
    "IMAGE_DIR = \"data/dataset/images\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8322aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CAPTION_FILE)\n",
    "df[\"image\"] = df[\"image\"].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
    "dataset = Dataset.from_pandas(df[[\"image\", \"caption\"]])\n",
    "split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset, eval_dataset = split[\"train\"], split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923762ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "\n",
    "def preprocess(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    caption = example[\"caption\"]\n",
    "    inputs = processor(images=image, text=\"Describe this image\", return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = processor.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
    "    return {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "eval_dataset = eval_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a95b83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"Salesforce/blip2-flan-t5-xl\"\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n",
    "peft_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q\", \"v\"], task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# üìä –ú–µ—Ç—Ä–∏–∫–∏\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_result[\"bleu\"], \"rougeL\": rouge_result[\"rougeL\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ac9d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîç –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "pre_model = Blip2ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "def get_metrics(model, dataset, processor):\n",
    "    inputs = [processor(images=Image.open(example[\"image\"]).convert(\"RGB\"), text=\"Describe this image\", return_tensors=\"pt\").to(model.device)\n",
    "              for example in dataset]\n",
    "    outputs = [model.generate(**inp, max_new_tokens=50)[0] for inp in inputs]\n",
    "    preds = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    targets = [example[\"caption\"] for example in dataset]\n",
    "    bleu_result = bleu.compute(predictions=preds, references=[[t] for t in targets])\n",
    "    rouge_result = rouge.compute(predictions=preds, references=targets)\n",
    "    return {\"bleu\": bleu_result[\"bleu\"], \"rougeL\": rouge_result[\"rougeL\"]}\n",
    "\n",
    "before_metrics = get_metrics(pre_model, eval_dataset, processor)\n",
    "print(\"–î–æ –æ–±—É—á–µ–Ω–∏—è:\", before_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de30ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    output_dir=\"./debug-out\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be30fd4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39506a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "after_metrics = get_metrics(model, eval_dataset, processor)\n",
    "print(\"–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\", after_metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
