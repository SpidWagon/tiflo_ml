{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca20f08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers peft datasets evaluate bitsandbytes torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bdf31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b90fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"Salesforce/blip2-flan-t5-xl\"\n",
    "processor = Blip2Processor.from_pretrained(model_id)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66621e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99912ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a692c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    image = example[\"image\"]\n",
    "    caption = example[\"sentence\"]\n",
    "    inputs = processor(images=image, text=\"Describe this image\", return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = processor.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
    "    return {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "eval_dataset = eval_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486acb05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_result[\"bleu\"], \"rougeL\": rouge_result[\"rougeL\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2e9d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    eval_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    output_dir=\"./blip2-xl-finetuned\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c461e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714575c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_examples(model, processor, dataset, num_images=3):\n",
    "    model.eval()\n",
    "    for i in range(num_images):\n",
    "        example = dataset[i]\n",
    "        image = example[\"image\"]\n",
    "        gt_caption = example[\"sentence\"]\n",
    "\n",
    "        inputs = processor(images=image, text=\"Describe this image\", return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(**inputs, max_new_tokens=50)\n",
    "        caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        display(image)\n",
    "        print(f\"üí¨ Ground Truth: {gt_caption}\")\n",
    "        print(f\"ü§ñ Model Output: {caption}\")\n",
    "        print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7243bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"–î–æ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "pretrained_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "show_examples(pretrained_model, processor, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dec9b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "show_examples(model, processor, eval_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
